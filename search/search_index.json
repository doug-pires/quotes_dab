{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Design Docs","text":""},{"location":"#overview","title":"Overview","text":"<p>Build a simple data pipeline that extract famous quotes, process it and save on storage.</p> <ul> <li>\ud83c\udfafGoals: Deploy Delta Live Tables &amp; Workflow pipeline into Databricks using Databricks Asset Bundles</li> <li>\u26a0\ufe0f Non-Goals : Be a complex pipeline</li> <li>\ud83c\udf1f Milestone: Follow best practices and principles</li> <li>\ud83e\udec2 Main Audience: Jr and Mid Data Enginners, and others interested parties</li> <li>\ud83d\udd25 Bonus : YAML file for CI/CD in GitHub Actions</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"Requirement No Requirement Summary 1 Use dab (Databricks Asset Bundles) 2 Apply Unit Testing on Python Code 3 Test Coverage be higher than 80% 4 Use Development Mix Mode ( Notebooks and Python Scripts) 5 Self document code using Docstrings 6 Design a Documentation in GitHub Pages"},{"location":"#design-considerations","title":"Design Considerations","text":""},{"location":"#data-sources","title":"Data sources","text":"<p>It's an API hosted by API Ninjas and the user should sign up to get an API Key.</p> <p>You can add the API KEY as <code>secrets</code> in Databricks-Backed Scope.</p> <p>The commands:</p> <pre><code>databricks secrets create-scope SCOPE_NAME\ndatabricks secrets put-secret SCOPE_NAME SECRET_NAME\n</code></pre> <p>Then will prompt a screen to add the secret.</p>"},{"location":"#data-ingestion","title":"Data Ingestion","text":"<p>Python code to request the quote and each request will get a new random quote. We can consider the  Data Volume small, being not a challenge in that use case.</p>"},{"location":"#data-storage","title":"Data Storage","text":"<p>For the sake of simplicity will be stored in DBFS</p>"},{"location":"#data-processing","title":"Data Processing","text":"<p>For further processing we will use Delta Live Tables</p> <ul> <li>Autoloader will ingest incrementally to a <code>streaming</code> Bronze Table, adding some metadata information to track the batch and auditing purposes.</li> <li>Silver table we will hash some columns to uniquely identify a quote.</li> <li>Gold Tables for aggregation and report purposes.</li> </ul>"},{"location":"#data-consumption","title":"Data Consumption","text":"<p>No Data Consumers waiting in downstream tools</p>"},{"location":"#data-operations","title":"Data Operations","text":"<p>The orchestration will be done by Databricks Workflows</p>"},{"location":"#data-governance","title":"Data Governance","text":"<p>Out-of-Scope \u274c</p>"},{"location":"#data-security","title":"Data Security","text":"<p>Out-of-Scope \u274c</p>"},{"location":"#tech-solution","title":"Tech Solution","text":""},{"location":"#workflow","title":"Workflow","text":""},{"location":"#architecture","title":"Architecture","text":""},{"location":"#manage-metadata-and-build-process","title":"Manage Metadata and Build Process:","text":"<ul> <li>Poetry</li> </ul>"},{"location":"#deploy-databricks-assets","title":"Deploy Databricks Assets","text":"<ul> <li>Databricks Asset Bundles</li> </ul>"},{"location":"#python-libraries","title":"Python Libraries:","text":"<ul> <li>pyspark</li> <li>delta-spark</li> <li>databricks-sdk</li> <li>requests</li> </ul>"},{"location":"#test","title":"Test","text":"<ul> <li>pytest</li> <li>chispa</li> <li>pytest-cov</li> <li>pytest-mock</li> </ul>"},{"location":"#linters","title":"Linters","text":"<ul> <li>isort</li> <li>black</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>mkdocs</li> <li>mkdocs-material</li> </ul>"},{"location":"common_functions/","title":"Common Functions","text":"<pre><code>Functions that I can use in any use case\n</code></pre>"},{"location":"common_functions/#example","title":"Example","text":""},{"location":"common_functions/#adding-a-hash-column-to-a-dataframe","title":"Adding a Hash Column to a DataFrame","text":"<p>In this example, we will demonstrate how to use the <code>add_hash_col</code> function to add a hash column to a DataFrame. We'll be using Apache Spark for DataFrame operations.</p>"},{"location":"common_functions/#function-description","title":"Function Description","text":"<p>The <code>add_hash_col</code> function is designed to add a new column to a DataFrame that stores the hash value of concatenated values from specified columns. It takes the following arguments:</p> <ul> <li><code>df</code> (DataFrame): The input DataFrame to which the new column will be added.</li> <li><code>cols_to_hash</code> (list of strings): A list of column names to be concatenated and hashed.</li> <li><code>new_col_name</code> (string, optional): The name of the new column to store the hash values. It defaults to \"hash_col.\"</li> </ul> <p>Here's the function code:</p> <pre><code>def add_hash_col(\n    df: DataFrame, cols_to_hash: list[str], new_col_name: str = \"hash_col\"\n) -&gt; DataFrame:\n    expr = [F.col(col_name) for col_name in cols_to_hash]\n\n    df = df.withColumn(new_col_name, F.md5(F.concat(*expr)))\n    return df\n\n\nfrom pyspark.sql import SparkSession\nfrom my_module import add_hash_col\n\n# Create a Spark session\nspark = SparkSession.builder.appName(\"DataFrameHashing\").getOrCreate()\n\n\n# Sample DataFrame\ndata = [(\"John\", \"Doe\", 30),\n        (\"Jane\", \"Smith\", 25),\n        (\"Bob\", \"Johnson\", 40)]\ncolumns = [\"first_name\", \"last_name\", \"age\"]\ndf = spark.createDataFrame(data, columns)\n\n# Columns to hash\ncols_to_hash = [\"first_name\", \"last_name\"]\n\n# Show the updated DataFrame\nhashed_df.show()\n</code></pre>"},{"location":"common_functions/#results","title":"Results","text":"first_name last_name age hash_col John Doe 30 6a02a05d57b8e18a6... Jane Smith 25 5875b470e6b0f35a2... Bob Johnson 40 caf76821891f74b2..."},{"location":"common_functions/#common-functions-source-code","title":"Common Functions -&gt; Source Code","text":""},{"location":"common_functions/#quotes_dab.common.add_hash_col","title":"<code>add_hash_col(df, cols_to_hash, new_col_name='hash_col')</code>","text":"<p>Add a new column to the DataFrame that contains the hash value of the concatenated values of specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to which the new column will be added.</p> required <code>cols_to_hash</code> <code>list[str]</code> <p>A list of column names to be concatenated and hashed.</p> required <code>new_col_name</code> <code>str</code> <p>The name of the new column to store the hash values. Defaults to \"hash_col\".</p> <code>'hash_col'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with an additional column containing the hash values.</p> Source code in <code>quotes_dab/common.py</code> <pre><code>def add_hash_col(\n    df: DataFrame, cols_to_hash: list[str], new_col_name: str = \"hash_col\"\n) -&gt; DataFrame:\n    \"\"\"\n    Add a new column to the DataFrame that contains the hash value of the concatenated values of specified columns.\n\n    Args:\n        df (DataFrame): The input DataFrame to which the new column will be added.\n        cols_to_hash (list[str]): A list of column names to be concatenated and hashed.\n        new_col_name (str, optional): The name of the new column to store the hash values. Defaults to \"hash_col\".\n\n    Returns:\n        DataFrame: A new DataFrame with an additional column containing the hash values.\n    \"\"\"\n    expr = [F.col(col_name) for col_name in cols_to_hash]\n\n    df = df.withColumn(new_col_name, F.md5(F.concat(*expr)))\n    return df\n</code></pre>"},{"location":"common_functions/#quotes_dab.common.add_metadata_cols","title":"<code>add_metadata_cols(df)</code>","text":"<p>Add metadata columns to a Spark DataFrame. Usually used in Databricks Autoloader https://docs.databricks.com/en/ingestion/file-metadata-column.html</p> <p>This function takes a Spark DataFrame and adds two columns, 'ingestion_datetime' and 'file_name', by extracting information from the '_metadata' column if it exists.</p> <p>Parameters: df (DataFrame): The input Spark DataFrame that may contain the '_metadata' column.</p> <p>Returns: DataFrame: A new Spark DataFrame with 'ingestion_datetime' and 'file_name' columns added if '_metadata' exists in the input DataFrame, or the original DataFrame if '_metadata' is not present.</p> Source code in <code>quotes_dab/common.py</code> <pre><code>def add_metadata_cols(df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Add metadata columns to a Spark DataFrame.\n    Usually used in Databricks Autoloader\n    https://docs.databricks.com/en/ingestion/file-metadata-column.html\n\n    This function takes a Spark DataFrame and adds two columns, 'ingestion_datetime'\n    and 'file_name', by extracting information from the '_metadata' column if it exists.\n\n    Parameters:\n    df (DataFrame): The input Spark DataFrame that may contain the '_metadata' column.\n\n    Returns:\n    DataFrame: A new Spark DataFrame with 'ingestion_datetime' and 'file_name' columns\n    added if '_metadata' exists in the input DataFrame, or the original DataFrame if\n    '_metadata' is not present.\n    \"\"\"\n    df = df.withColumn(\n        \"file_modification_time\", F.col(\"_metadata.file_modification_time\")\n    ).withColumn(\"file_name\", F.col(\"_metadata.file_name\"))\n\n    return df\n</code></pre>"},{"location":"common_functions/#quotes_dab.common.cast_cols","title":"<code>cast_cols(df, cols_to_cast)</code>","text":"<p>Casts specified columns to the provided data types in a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <code>cols_to_cast</code> <code>dict[str, str]</code> <p>A dictionary where keys are column names and values are the target data types for casting.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with specified columns cast to the target data types.</p> Example <p>You can use this function to cast specific columns in a DataFrame.</p> <pre><code>from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\ndata = [(\"John\", \"30\", \"true\"), (\"Alice\", \"25\", \"false\")]\ncolumns = [\"name\", \"age\", \"is_student\"]\ndf = spark.createDataFrame(data, columns)\ndf.show()\n+-----+---+----------+\n| name|age|is_student|\n+-----+---+----------+\n| John| 30| true|\n|Alice| 25| false|\n+-----+---+----------+\n\ncast_types = {\"age\": \"int\", \"is_student\": \"boolean\"}\nnew_df = cast_cols(df, cast_types)\nnew_df.show()\n+-----+---+----------+\n| name|age|is_student|\n+-----+---+----------+\n| John| 30| true|\n|Alice| 25| false|\n+-----+---+----------+\n</code></pre> Source code in <code>quotes_dab/common.py</code> <pre><code>def cast_cols(df: DataFrame, cols_to_cast: dict[str, str]) -&gt; DataFrame:\n    \"\"\"\n    Casts specified columns to the provided data types in a DataFrame.\n\n    Args:\n        df (DataFrame): The input DataFrame.\n\n        cols_to_cast (dict[str, str]): A dictionary where keys are column names\n            and values are the target data types for casting.\n\n    Returns:\n        DataFrame: A new DataFrame with specified columns cast to the target data types.\n\n    Example:\n        You can use this function to cast specific columns in a DataFrame.\n        ```python\n        from pyspark.sql import SparkSession\n        spark = SparkSession.builder.appName(\"example\").getOrCreate()\n        data = [(\"John\", \"30\", \"true\"), (\"Alice\", \"25\", \"false\")]\n        columns = [\"name\", \"age\", \"is_student\"]\n        df = spark.createDataFrame(data, columns)\n        df.show()\n        +-----+---+----------+\n        | name|age|is_student|\n        +-----+---+----------+\n        | John| 30| true|\n        |Alice| 25| false|\n        +-----+---+----------+\n\n        cast_types = {\"age\": \"int\", \"is_student\": \"boolean\"}\n        new_df = cast_cols(df, cast_types)\n        new_df.show()\n        +-----+---+----------+\n        | name|age|is_student|\n        +-----+---+----------+\n        | John| 30| true|\n        |Alice| 25| false|\n        +-----+---+----------+\n        ```\n\n    \"\"\"\n    cols = list(cols_to_cast.keys())\n\n    for col in cols:\n        type = cols_to_cast.get(col)\n        df = df.withColumn(col, F.col(col).cast(type))\n    return df\n</code></pre>"},{"location":"common_functions/#quotes_dab.common.drop_columns","title":"<code>drop_columns(df, cols_to_drop)</code>","text":"<p>Drops specified columns from a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <code>cols_to_drop</code> <code>list[str]</code> <p>List of column names to be dropped.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with specified columns removed.</p> Source code in <code>quotes_dab/common.py</code> <pre><code>def drop_columns(df: DataFrame, cols_to_drop: list[str]) -&gt; DataFrame:\n    \"\"\"\n    Drops specified columns from a DataFrame.\n\n    Args:\n        df (DataFrame): The input DataFrame.\n        cols_to_drop (list[str]): List of column names to be dropped.\n\n    Returns:\n        DataFrame: A new DataFrame with specified columns removed.\n    \"\"\"\n    df = df.drop(*cols_to_drop)\n    return df\n</code></pre>"},{"location":"common_functions/#quotes_dab.common.group_by_counting_rows","title":"<code>group_by_counting_rows(df, col)</code>","text":"<p>Group a DataFrame by a specified column and count the occurrences of each group.</p> <p>Parameters: df (DataFrame): The input DataFrame to be grouped. col (str): The name of the column by which to group the DataFrame.</p> <p>Returns: DataFrame: A new DataFrame with the groups and their corresponding counts.</p> Source code in <code>quotes_dab/common.py</code> <pre><code>def group_by_counting_rows(df: DataFrame, col: str):\n    \"\"\"\n    Group a DataFrame by a specified column and count the occurrences of each group.\n\n    Parameters:\n    df (DataFrame): The input DataFrame to be grouped.\n    col (str): The name of the column by which to group the DataFrame.\n\n    Returns:\n    DataFrame: A new DataFrame with the groups and their corresponding counts.\n    \"\"\"\n    df_grouped = df.groupBy(col).count()\n    return df_grouped\n</code></pre>"},{"location":"logging/","title":"Logging Configuration","text":""},{"location":"logging/#quotes_dab.config_logging.get_stream_handler","title":"<code>get_stream_handler()</code>","text":"<p>Create and configure a logging StreamHandler with a specific formatter and log level.</p> <p>Returns:</p> Type Description <code>StreamHandler</code> <p>logging.StreamHandler: An instance of logging.StreamHandler configured with the specified formatter and log level.</p> Example <pre><code>handler = get_stream_handler()\nlogger = logging.getLogger(__name__)\nlogger.addHandler(handler)\nlogger.setLevel(logging.INFO)\n</code></pre> <p>This function creates a logging StreamHandler, which directs log messages to the console (stdout). It configures the StreamHandler with a predefined formatter and sets the log level to INFO by default.</p> <p>Returns the configured StreamHandler, ready to be added to a logger for handling log messages.</p> Source code in <code>quotes_dab/config_logging.py</code> <pre><code>def get_stream_handler() -&gt; logging.StreamHandler:\n    \"\"\"\n    Create and configure a logging StreamHandler with a specific formatter and log level.\n\n    Returns:\n        logging.StreamHandler: An instance of logging.StreamHandler configured with the specified formatter\n            and log level.\n\n    Example:\n        ```python\n        handler = get_stream_handler()\n        logger = logging.getLogger(__name__)\n        logger.addHandler(handler)\n        logger.setLevel(logging.INFO)\n        ```\n\n    This function creates a logging StreamHandler, which directs log messages to the console (stdout).\n    It configures the StreamHandler with a predefined formatter and sets the log level to INFO by default.\n\n    Returns the configured StreamHandler, ready to be added to a logger for handling log messages.\n    \"\"\"\n    stream_handler = logging.StreamHandler()\n    # Add Formatter to Handlers\n    stream_handler.setFormatter(formatter)\n    stream_handler.setLevel(logging.INFO)\n    return stream_handler\n</code></pre>"},{"location":"logging/#quotes_dab.config_logging.get_stream_logger","title":"<code>get_stream_logger(logger_name)</code>","text":"<p>Create and configure a logging logger with a specific name, log level, and a StreamHandler.</p> <p>Parameters:</p> Name Type Description Default <code>logger_name</code> <code>str</code> <p>The name of the logger.</p> required <p>Returns:</p> Type Description <code>Logger</code> <p>logging.Logger: A Logger instance configured with the specified name, log level, and a StreamHandler.</p> Example <pre><code>    logger = get_logger('my_logger')\n    logger.info('This is an informational message')\n    logger.error('This is an error message')\n</code></pre> Source code in <code>quotes_dab/config_logging.py</code> <pre><code>def get_stream_logger(logger_name: str) -&gt; logging.Logger:\n    \"\"\"\n    Create and configure a logging logger with a specific name, log level, and a StreamHandler.\n\n    Args:\n        logger_name (str): The name of the logger.\n\n    Returns:\n        logging.Logger: A Logger instance configured with the specified name, log level, and a StreamHandler.\n\n    Example:\n        ```python3\n            logger = get_logger('my_logger')\n            logger.info('This is an informational message')\n            logger.error('This is an error message')\n        ```\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    logger.setLevel(logging.INFO)\n    # Add the Handlers to Logger\n    logger.addHandler(get_stream_handler())\n    return logger\n</code></pre>"},{"location":"references/","title":"Gems References \ud83d\udc8e","text":"<p>Alex Ott : Applying software development &amp; DevOps best practices to Delta Live Table pipelines | Databricks Blog</p> <p>Matthew Powers: Testing PySpark Code - MungingData</p> <p>Architecture by Databricks: What is Medallion or Multi-hop architecture?</p> <p>Gaurav Thalpati: Design Docs for Data Platforms</p> <p>The Pragmatic Engineer: RFC's &amp; Design Docs</p> <p>Abraham Pabbathi: Set-up Service Principal to run Databricks Jobs</p> <p>Brian Okken : Python Testing with pytest, 2nd Edition</p> <p>Eduardo Mendes : (6) Documentado projetos com MkDocs - Live de Python #189 - YouTube</p>"},{"location":"tests/","title":"Tests \ud83e\uddea","text":""},{"location":"tests/#unit","title":"Unit","text":""},{"location":"tests/#for-common-functions","title":"For Common Functions","text":""},{"location":"tests/#tests.unit_local.test_common.test_function_to_extract_metadata_from_dataframe","title":"<code>test_function_to_extract_metadata_from_dataframe(spark_session, dummy_metadata_data, expected_metadata_columns)</code>","text":"<p>Given the Data and Schema from Dummy Data containing Metadata, we create the Dataframe 'Countries' containing Metadata.</p> <p>When we call the 'add_metadata_cols' function to add columns for extracting metadata,</p> <p>Then we should assert that the expected metadata columns were added to the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark_session</code> <code>fixture</code> <p>A PySpark session used for DataFrame creation.</p> required <code>dummy_metadata_data</code> <code>fixture</code> <p>A fixture providing data and schema with a column as MapType()</p> required <code>expected_metadata_columns</code> <code>fixture</code> <p>A fixture containing the metadata columns I want to extract</p> required Source code in <code>tests/unit_local/test_common.py</code> <pre><code>@pytest.mark.parametrize(\n    \"expected_metadata_columns\", [\"file_name\", \"file_modification_time\"]\n)\ndef test_function_to_extract_metadata_from_dataframe(\n    spark_session: fixture,\n    dummy_metadata_data: fixture,\n    expected_metadata_columns: fixture,\n):\n    \"\"\"\n    Given the Data and Schema from Dummy Data containing Metadata,\n    we create the Dataframe 'Countries' containing Metadata.\n\n    When we call the 'add_metadata_cols' function to add columns for extracting metadata,\n\n    Then we should assert that the expected metadata columns were added to the DataFrame.\n\n    Args:\n        spark_session: A PySpark session used for DataFrame creation.\n        dummy_metadata_data: A fixture providing data and schema with a column as MapType()\n        expected_metadata_columns: A fixture containing the metadata columns I want to extract\n    \"\"\"\n    schema = dummy_metadata_data[0]\n    data = dummy_metadata_data[1]\n    df_countries = spark_session.createDataFrame(data=data, schema=schema)\n\n    df_with_metadata = df_countries.transform(add_metadata_cols)\n\n    assert expected_metadata_columns in df_with_metadata.columns\n</code></pre>"},{"location":"tests/#tests.unit_local.test_common.test_group_by_function_to_agg_by_specific_column","title":"<code>test_group_by_function_to_agg_by_specific_column(dummy_data, spark_session)</code>","text":"<p>Test the 'group_by_counting_rows' function to aggregate rows by 'country' and compare the result to the expected aggregated DataFrame.</p> <p>Steps: 1. Given the 'df_persons' DataFrame. 2. We expect to have an aggregated DataFrame by 'country' with specific schema. 3. When we call the function to group by 'country'. 4. The result should be a DataFrame with the same structure as 'df_expected_agg_by_country'. 5. We perform a DataFrame equality check, ignoring nullable and row order.</p> <p>Parameters:</p> Name Type Description Default <code>dummy_data</code> <code>fixture</code> <p>Fixture providing data and schema.</p> required <code>spark_session</code> <code>fixture</code> <p>PySpark session for DataFrame creation.</p> required Source code in <code>tests/unit_local/test_common.py</code> <pre><code>def test_group_by_function_to_agg_by_specific_column(\n    dummy_data: fixture, spark_session: fixture\n):\n    \"\"\"\n    Test the 'group_by_counting_rows' function to aggregate rows by 'country' and compare the result to the expected aggregated DataFrame.\n\n    Steps:\n    1. Given the 'df_persons' DataFrame.\n    2. We expect to have an aggregated DataFrame by 'country' with specific schema.\n    3. When we call the function to group by 'country'.\n    4. The result should be a DataFrame with the same structure as 'df_expected_agg_by_country'.\n    5. We perform a DataFrame equality check, ignoring nullable and row order.\n\n    Args:\n        dummy_data: Fixture providing data and schema.\n        spark_session: PySpark session for DataFrame creation.\n    \"\"\"\n    schema = dummy_data[0]\n    data = dummy_data[1]\n    df_persons = spark_session.createDataFrame(data=data, schema=schema)\n\n    expected_agg_fields = [\n        StructField(\"country\", StringType(), nullable=True),\n        StructField(\"count\", LongType(), nullable=True),\n    ]\n    schema_df_agg_by_country = StructType(expected_agg_fields)\n\n    data_country_agg = [(\"Germany\", 2), (\"Italy\", 1), (\"Brazil\", 1)]\n\n    df_expected_agg_by_country = spark_session.createDataFrame(\n        data=data_country_agg, schema=schema_df_agg_by_country\n    )\n\n    df_grouped_by_country = group_by_counting_rows(df=df_persons, col=\"country\")\n\n    assert_df_equality(\n        df_expected_agg_by_country,\n        df_grouped_by_country,\n        ignore_nullable=True,\n        ignore_row_order=True,\n    )\n</code></pre>"},{"location":"tests/#tests.unit_local.test_common.test_if_columns_provided_was_casted","title":"<code>test_if_columns_provided_was_casted(spark_session, dummy_data)</code>","text":"<p>Given the Data and Schema from Dummy Data, we create two DataFrames:  1. 'df_persons_correct_schema' with the CORRECT SCHEMA.  2. 'df_wrong_schema' with the WRONG SCHEMA containing all STRINGTYPE columns.</p> <p>When we call the 'cast_cols' function to cast the specified columns on the 'df_wrong_schema',</p> <p>Then it should return a DataFrame with the expected schema, as in 'df_persons_correct_schema',  using Chispa for schema comparison.</p> <p>Parameters:</p> Name Type Description Default <code>spark_session</code> <code>fixture</code> <p>A PySpark session used for DataFrame creation.</p> required <code>dummy_data</code> <code>fixture</code> <p>A fixture providing data and schema.</p> required Source code in <code>tests/unit_local/test_common.py</code> <pre><code>def test_if_columns_provided_was_casted(spark_session: fixture, dummy_data: fixture):\n    \"\"\"\n     Given the Data and Schema from Dummy Data, we create two DataFrames:\n     1. 'df_persons_correct_schema' with the CORRECT SCHEMA.\n     2. 'df_wrong_schema' with the WRONG SCHEMA containing all STRINGTYPE columns.\n\n     When we call the 'cast_cols' function to cast the specified columns on the 'df_wrong_schema',\n\n     Then it should return a DataFrame with the expected schema, as in 'df_persons_correct_schema',\n     using Chispa for schema comparison.\n\n    Args:\n        spark_session: A PySpark session used for DataFrame creation.\n        dummy_data: A fixture providing data and schema.\n    \"\"\"\n    schema = dummy_data[0]\n    data = dummy_data[1]\n\n    df_persons_correct_schema = spark_session.createDataFrame(data=data, schema=schema)\n\n    fields_wrong = [\n        StructField(\"name\", StringType(), nullable=True),\n        StructField(\"age\", StringType(), nullable=True),\n        StructField(\"job\", StringType(), nullable=True),\n        StructField(\"country\", StringType(), nullable=True),\n        StructField(\"is_married\", StringType(), nullable=True),\n    ]\n\n    schema_wrong = StructType(fields_wrong)\n    df_wrong_schema = spark_session.createDataFrame(data=data, schema=schema_wrong)\n\n    cols_to_be_casted = {\"age\": \"int\", \"is_married\": \"boolean\"}\n\n    df_fixed = df_wrong_schema.transform(cast_cols, cols_to_cast=cols_to_be_casted)\n\n    assert_df_equality(df_persons_correct_schema, df_fixed)\n</code></pre>"},{"location":"tests/#tests.unit_local.test_common.test_if_function_dropped_the_list_columns","title":"<code>test_if_function_dropped_the_list_columns(spark_session, dummy_data)</code>","text":"<p>Given the Data and Schema from Dummy Data, we create the Dataframe 'Persons' and specify a list of columns to drop.</p> <p>When we call the 'drop_columns' function on a DataFrame, the specified columns should be removed.</p> <p>Then the resulting DataFrame should only contain the expected columns.</p> <p>Parameters:</p> Name Type Description Default <code>spark_session</code> <code>fixture</code> <p>A PySpark session used for DataFrame creation.</p> required <code>dummy_data</code> <code>fixture</code> <p>A fixture providing data and schema.</p> required Source code in <code>tests/unit_local/test_common.py</code> <pre><code>def test_if_function_dropped_the_list_columns(\n    spark_session: fixture, dummy_data: fixture\n):\n    \"\"\"\n     Given the Data and Schema from Dummy Data, we create the Dataframe 'Persons' and specify a list of columns to drop.\n\n     When we call the 'drop_columns' function on a DataFrame, the specified columns should be removed.\n\n     Then the resulting DataFrame should only contain the expected columns.\n\n    Args:\n        spark_session: A PySpark session used for DataFrame creation.\n        dummy_data: A fixture providing data and schema.\n    \"\"\"\n    schema = dummy_data[0]\n    data = dummy_data[1]\n    df_persons = spark_session.createDataFrame(data=data, schema=schema)\n\n    cols_to_be_removed = [\"job\", \"city\"]\n\n    df_dropped_cols = df_persons.transform(\n        drop_columns, cols_to_drop=cols_to_be_removed\n    )\n\n    assert cols_to_be_removed not in df_dropped_cols.columns\n</code></pre>"},{"location":"tests/#tests.unit_local.test_common.test_if_hash_col_was_created","title":"<code>test_if_hash_col_was_created(dummy_data, spark_session)</code>","text":"<p>Test the 'add_hash_col' function to check if a 'hash_col' was created and contains the expected values.</p> <p>Steps: 1. Given the Data and Schema from Dummy Data, we create the Dataframe 'df_persons'. 2. When the first row containing values 'Douglas | 31 | Engineer | Brazil | True' is hashed using md5. 3. We call the function to create a 'hash_col' and hash the specified columns. 4. Then we should have a column named 'hash_col' in the resulting DataFrame. 5. We check the first hash value for the values of the first row against the expected hash value.</p> <p>Parameters:</p> Name Type Description Default <code>dummy_data</code> <code>fixture</code> <p>Fixture providing data and schema.</p> required <code>spark_session</code> <code>fixture</code> <p>PySpark session for DataFrame creation.</p> required Source code in <code>tests/unit_local/test_common.py</code> <pre><code>def test_if_hash_col_was_created(dummy_data: fixture, spark_session: fixture):\n    \"\"\"\n    Test the 'add_hash_col' function to check if a 'hash_col' was created and contains the expected values.\n\n    Steps:\n    1. Given the Data and Schema from Dummy Data, we create the Dataframe 'df_persons'.\n    2. When the first row containing values 'Douglas | 31 | Engineer | Brazil | True' is hashed using md5.\n    3. We call the function to create a 'hash_col' and hash the specified columns.\n    4. Then we should have a column named 'hash_col' in the resulting DataFrame.\n    5. We check the first hash value for the values of the first row against the expected hash value.\n\n    Args:\n        dummy_data: Fixture providing data and schema.\n        spark_session: PySpark session for DataFrame creation.\n    \"\"\"\n    schema = dummy_data[0]\n    data = dummy_data[1]\n    df_persons = spark_session.createDataFrame(data=data, schema=schema)\n\n    row_1 = \"Douglas31EngineerBraziltrue\"\n    expected_hash = hashlib.md5(row_1.encode()).hexdigest()\n\n    cols_to_be_hashed = df_persons.columns\n    df_w_col_hashed = df_persons.transform(add_hash_col, cols_to_hash=cols_to_be_hashed)\n\n    assert \"hash_col\" in df_w_col_hashed.columns\n    assert expected_hash == df_w_col_hashed.collect()[0][\"hash_col\"]\n</code></pre>"},{"location":"tests/#for-extract-functions","title":"For Extract Functions","text":""},{"location":"tests/#tests.unit_local.test_extract.test_if_data_is_none_will_not_save_to_storage_and_let_user_know","title":"<code>test_if_data_is_none_will_not_save_to_storage_and_let_user_know(caplog, mock_authentication_databricks)</code>","text":"<p>Test the 'save_to_storage' function to ensure that it does not save data to storage when the data is None and logs a message to inform the user.</p> <p>Parameters:</p> Name Type Description Default <code>caplog</code> <code>fixture</code> <p>A fixture for capturing log messages.</p> required <code>mock_authentication_databricks</code> <code>fixture</code> <p>A fixture for mocking the 'authenticate_databricks' function.</p> required <p>Given: - The 'data' is set to None. - The 'path' is specified as '/mnt/fake/path'.</p> <p>When: - We call the 'save_to_storage' function with the provided parameters.</p> <p>Then: - The code should return a log message indicating that the 'data' is None.</p> Source code in <code>tests/unit_local/test_extract.py</code> <pre><code>def test_if_data_is_none_will_not_save_to_storage_and_let_user_know(\n    caplog: fixture, mock_authentication_databricks: fixture\n):\n    \"\"\"\n    Test the 'save_to_storage' function to ensure that it does not save data to storage when the data is None and logs a message to inform the user.\n\n    Args:\n        caplog (fixture): A fixture for capturing log messages.\n        mock_authentication_databricks (fixture): A fixture for mocking the 'authenticate_databricks' function.\n\n    Given:\n    - The 'data' is set to None.\n    - The 'path' is specified as '/mnt/fake/path'.\n\n    When:\n    - We call the 'save_to_storage' function with the provided parameters.\n\n    Then:\n    - The code should return a log message indicating that the 'data' is None.\n    \"\"\"\n    data = None\n    path = \"/mnt/fake/path\"\n\n    w_mock = mock_authentication_databricks\n\n    save_to_storage(workspace=w_mock, data=data, path_dbfs=path)\n\n    expected_log_message = \"Data returned None\"\n    result_log = caplog.text\n    assert expected_log_message in result_log\n</code></pre>"},{"location":"tests/#tests.unit_local.test_extract.test_if_quote_return_string_when_success","title":"<code>test_if_quote_return_string_when_success(mocker, api_key_mock_local)</code>","text":"<p>Test the 'extract_quote' function to ensure it returns a list of quotes when the API request is successful.</p> <p>Parameters:</p> Name Type Description Default <code>mocker</code> <code>fixture</code> <p>A Pytest fixture for mocking external dependencies.</p> required <code>api_key_mock_local</code> <code>fixture</code> <p>A fixture representing the API key used for the test.</p> required <p>Given: - The status code of success (200) is simulated in 'mock_response'. - The 'mock_response' object is configured to return a list of quotes.</p> <p>When: - We call the 'extract_quote' function while running locally with 'api_key_mock_local'.</p> <p>Then: - The result 'quote' should be of type 'list'.</p> Source code in <code>tests/unit_local/test_extract.py</code> <pre><code>def test_if_quote_return_string_when_success(\n    mocker: fixture, api_key_mock_local: fixture\n):\n    \"\"\"\n    Test the 'extract_quote' function to ensure it returns a list of quotes when the API request is successful.\n\n    Args:\n        mocker (fixture): A Pytest fixture for mocking external dependencies.\n        api_key_mock_local (fixture): A fixture representing the API key used for the test.\n\n    Given:\n    - The status code of success (200) is simulated in 'mock_response'.\n    - The 'mock_response' object is configured to return a list of quotes.\n\n    When:\n    - We call the 'extract_quote' function while running locally with 'api_key_mock_local'.\n\n    Then:\n    - The result 'quote' should be of type 'list'.\n    \"\"\"\n    mock_response = mocker.Mock()\n    mock_response.status_code = 200\n    mock_response.json.return_value = [\"This is a quote\"]\n    mocker.patch(\"requests.get\", return_value=mock_response)\n\n    api_key = api_key_mock_local\n\n    quote = extract_quote(API_KEY=api_key)\n\n    assert isinstance(quote, list)\n</code></pre>"},{"location":"tests/#tests.unit_local.test_extract.test_log_error_when_function_return_400","title":"<code>test_log_error_when_function_return_400(mocker, caplog, api_key_mock_local)</code>","text":"<p>Test the 'extract_quote' function to log an error message when the API request returns a 400 status code.</p> <p>Parameters:</p> Name Type Description Default <code>mocker</code> <code>fixture</code> <p>A Pytest fixture for mocking external dependencies.</p> required <code>caplog</code> <code>fixture</code> <p>A fixture for capturing log messages.</p> required <code>api_key_mock_local</code> <code>fixture</code> <p>A fixture representing the API key used for the test.</p> required <p>Given: - The response is mocked to return a 400 status code with the text \"Bad Request\".</p> <p>When: - We call the 'extract_quote' function while running locally with 'api_key_mock_local', which generates the wrong status code.</p> <p>Then: - The log message should be checked to ensure it contains the expected error message, which includes the status code and reason.</p> Source code in <code>tests/unit_local/test_extract.py</code> <pre><code>def test_log_error_when_function_return_400(\n    mocker: fixture, caplog: fixture, api_key_mock_local: fixture\n):\n    \"\"\"\n    Test the 'extract_quote' function to log an error message when the API request returns a 400 status code.\n\n    Args:\n        mocker (fixture): A Pytest fixture for mocking external dependencies.\n        caplog (fixture): A fixture for capturing log messages.\n        api_key_mock_local (fixture): A fixture representing the API key used for the test.\n\n    Given:\n    - The response is mocked to return a 400 status code with the text \"Bad Request\".\n\n    When:\n    - We call the 'extract_quote' function while running locally with 'api_key_mock_local', which generates the wrong status code.\n\n    Then:\n    - The log message should be checked to ensure it contains the expected error message, which includes the status code and reason.\n    \"\"\"\n    # Given the response returning 400, because we are mocking it\n    mock_response = mocker.Mock()\n    mock_response.status_code = 400\n    mock_response.text = \"Bad Request\"\n    mocker.patch(\"requests.get\", return_value=mock_response)\n\n    api_key = api_key_mock_local\n\n    extract_quote(API_KEY=api_key)\n\n    # Then the log message should be checked\n    expected_log_message = (\n        f\"Status Code: {mock_response.status_code} - Reason: {mock_response.text}\"\n    )\n    assert expected_log_message in caplog.text\n</code></pre>"},{"location":"tests/#tests.unit_local.test_extract.test_return_one_string_randomly_between_list_of_categories","title":"<code>test_return_one_string_randomly_between_list_of_categories()</code>","text":"<p>Test the 'pick_random_category' function to ensure it selects a random string from a list of categories.</p> <p>Steps: 1. Given a list of countries in 'country_list'. 2. When we call the 'pick_random_category' function to select one randomly. 3. Then it should select one value that is in the 'country_list'. 4. Additionally, the selected value should be of type 'str'.</p> Source code in <code>tests/unit_local/test_extract.py</code> <pre><code>def test_return_one_string_randomly_between_list_of_categories():\n    \"\"\"\n    Test the 'pick_random_category' function to ensure it selects a random string from a list of categories.\n\n    Steps:\n    1. Given a list of countries in 'country_list'.\n    2. When we call the 'pick_random_category' function to select one randomly.\n    3. Then it should select one value that is in the 'country_list'.\n    4. Additionally, the selected value should be of type 'str'.\n    \"\"\"\n    country_list = [\"Germany\", \"Brazil\", \"Portugal\", \"India\"]\n\n    random_country = pick_random_category(country_list)\n\n    assert random_country in country_list\n    assert isinstance(random_country, str)\n</code></pre>"},{"location":"tests/#for-dummy-function","title":"For Dummy Function","text":""},{"location":"tests/#tests.unit_local.test_dummy.test_dummy_func","title":"<code>test_dummy_func()</code>","text":"<p>Test the dummy_func function.</p> <p>Given: - The expected return from my dummy func is \"Dummy.\"</p> <p>When: - I call the function to return the Dummy text.</p> <p>Then: - The result should be the same as the expected message.</p> Source code in <code>tests/unit_local/test_dummy.py</code> <pre><code>def test_dummy_func():\n    \"\"\"\n    Test the dummy_func function.\n\n    Given:\n    - The expected return from my dummy func is \"Dummy.\"\n\n    When:\n    - I call the function to return the Dummy text.\n\n    Then:\n    - The result should be the same as the expected message.\n    \"\"\"\n    expected_message = \"Dummy\"\n\n    result = dummy_func()\n\n    assert expected_message == result\n</code></pre>"},{"location":"tests/#integration","title":"Integration","text":""},{"location":"tests/#for-api-ninjas","title":"For API Ninjas","text":""},{"location":"tests/#tests.integration.test_api_ninjas.test_integration_with_api_ninjas","title":"<code>test_integration_with_api_ninjas(api_key_integration)</code>","text":"<p>Test the integration with the Ninjas API using a real API key.</p> <p>This integration test verifies that the function 'extract_quote' successfully connects to the Ninjas API using the provided real API key and returns a real quote from the API.</p> <p>Parameters:</p> Name Type Description Default <code>api_key_integration</code> <code>str</code> <p>A real API key obtained from environment variables.</p> required <p>Integration Test Steps: Given: - The REAL API key coming from environment variables. - The integration test is running in a CI environment (GitHub Actions).</p> <p>When: - I call the function 'extract_quote' with the real API key.</p> <p>Then: - The function should return a list containing at least one dictionary. - The 'quote' field in the returned dictionary must be a string.</p> <p>This integration test ensures that the 'extract_quote' function works correctly when interacting with the Ninjas API and handles the API key properly, especially in a CI environment.</p> <p>Note: This integration test is designed to run in a CI environment, such as GitHub Actions, where it can safely use a real API key for testing.</p> Source code in <code>tests/integration/test_api_ninjas.py</code> <pre><code>def test_integration_with_api_ninjas(api_key_integration: fixture):\n    \"\"\"\n    Test the integration with the Ninjas API using a real API key.\n\n    This integration test verifies that the function 'extract_quote' successfully connects to the\n    Ninjas API using the provided real API key and returns a real quote from the API.\n\n    Args:\n        api_key_integration (str): A real API key obtained from environment variables.\n\n    Integration Test Steps:\n    Given:\n    - The REAL API key coming from environment variables.\n    - The integration test is running in a CI environment (GitHub Actions).\n\n    When:\n    - I call the function 'extract_quote' with the real API key.\n\n    Then:\n    - The function should return a list containing at least one dictionary.\n    - The 'quote' field in the returned dictionary must be a string.\n\n    This integration test ensures that the 'extract_quote' function works correctly when interacting with\n    the Ninjas API and handles the API key properly, especially in a CI environment.\n\n    Note: This integration test is designed to run in a CI environment, such as GitHub Actions,\n    where it can safely use a real API key for testing.\n    \"\"\"\n    real_api_key = api_key_integration\n    real_quote = extract_quote(API_KEY=real_api_key)\n    quote = real_quote[0].get(\"quote\")\n\n    # Assertions\n    assert isinstance(real_quote, list)\n    assert isinstance(quote, str)\n</code></pre>"},{"location":"use_case_functions/","title":"Functions specific for the Use Case Quotes","text":""},{"location":"use_case_functions/#quotes_dab.request_quote.authenticate_databricks","title":"<code>authenticate_databricks()</code>","text":"<p>Authenticate with Databricks using a specified profile.</p> <p>This function creates a WorkspaceClient to authenticate with Databricks using the given profile</p> Source code in <code>quotes_dab/request_quote.py</code> <pre><code>def authenticate_databricks() -&gt; WorkspaceClient:\n    \"\"\"\n    Authenticate with Databricks using a specified profile.\n\n    This function creates a WorkspaceClient to authenticate with Databricks using the given profile\n    \"\"\"\n    w = WorkspaceClient(profile=profile_to_authenticate)\n    return w\n</code></pre>"},{"location":"use_case_functions/#quotes_dab.request_quote.extract_quote","title":"<code>extract_quote(API_KEY)</code>","text":"<p>Extract a random quote from an API and return it as a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>API_KEY</code> <code>str</code> <p>The API key for authentication.</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of dictionaries containing the extracted quote.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Any exception raised during the API request.</p> Note <p>This function makes an API request to retrieve a random quote based on a selected category.</p> Source code in <code>quotes_dab/request_quote.py</code> <pre><code>def extract_quote(API_KEY) -&gt; list[dict]:\n    \"\"\"\n    Extract a random quote from an API and return it as a list of dictionaries.\n\n    Args:\n        API_KEY (str): The API key for authentication.\n\n    Returns:\n        list[dict]: A list of dictionaries containing the extracted quote.\n\n    Raises:\n        Exception: Any exception raised during the API request.\n\n    Note:\n        This function makes an API request to retrieve a random quote based on a selected category.\n    \"\"\"\n    category = pick_random_category(\n        category_list\n    )  # Assuming category_list is defined elsewhere\n    api_url = f\"https://api.api-ninjas.com/v1/quotes?category={category}\"\n    response = requests.get(api_url, headers={\"X-Api-Key\": API_KEY})\n\n    if response.status_code == requests.codes.ok:\n        quote = response.json()\n        logger.info(\"Extracted quote: %s\", quote)\n        return quote\n    else:\n        logger.error(\n            \"Status Code: %s - Reason: %s\", response.status_code, response.text\n        )\n</code></pre>"},{"location":"use_case_functions/#quotes_dab.request_quote.get_api_key","title":"<code>get_api_key(workspace)</code>","text":"<p>Retrieve an API key from Databricks secrets.</p> <p>Parameters:</p> Name Type Description Default <code>workspace</code> <code>WorkspaceClient</code> <p>An instance of the Databricks WorkspaceClient.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The API key retrieved from Databricks secrets.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the API key is not found in the specified secrets scope.</p> <p>Example:</p> <pre><code>from databricks import WorkspaceClient\napi_key = get_api_key(WorkspaceClient())\nprint(f\"API Key: {api_key}\")\n</code></pre> Source code in <code>quotes_dab/request_quote.py</code> <pre><code>def get_api_key(workspace: WorkspaceClient) -&gt; str:\n    \"\"\"\n    Retrieve an API key from Databricks secrets.\n\n    Args:\n        workspace (WorkspaceClient): An instance of the Databricks WorkspaceClient.\n\n    Returns:\n        str: The API key retrieved from Databricks secrets.\n\n    Raises:\n        KeyError: If the API key is not found in the specified secrets scope.\n\n    Example:\n    ```python\n    from databricks import WorkspaceClient\n    api_key = get_api_key(WorkspaceClient())\n    print(f\"API Key: {api_key}\")\n    ```\n    \"\"\"\n    # Get API Key\n    API_KEY = workspace.dbutils.secrets.get(scope=\"api_keys\", key=\"ninjas\")\n    return API_KEY\n</code></pre>"},{"location":"use_case_functions/#quotes_dab.request_quote.pick_random_category","title":"<code>pick_random_category(words_of_list)</code>","text":"<p>Pick a random category from the provided list of words.</p> <p>Parameters:</p> Name Type Description Default <code>words_of_list</code> <code>list</code> <p>A list of categories from which a random category will be chosen.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A randomly selected category from the list.</p> <p>This function takes a list of categories and selects a random category from the list. It returns the chosen category as a string.</p> Example <p>category = pick_random_category([\"age\", \"alone\", \"amazing\"])</p> Source code in <code>quotes_dab/request_quote.py</code> <pre><code>def pick_random_category(words_of_list: list) -&gt; str:\n    \"\"\"\n    Pick a random category from the provided list of words.\n\n    Args:\n        words_of_list (list): A list of categories from which a random category will be chosen.\n\n    Returns:\n        str: A randomly selected category from the list.\n\n    This function takes a list of categories and selects a random category from the list.\n    It returns the chosen category as a string.\n\n    Example:\n        category = pick_random_category([\"age\", \"alone\", \"amazing\"])\n    \"\"\"\n    return rd.choice(words_of_list)\n</code></pre>"},{"location":"use_case_functions/#quotes_dab.request_quote.save_to_storage","title":"<code>save_to_storage(workspace, path_dbfs, data)</code>","text":"<p>Save data as a JSON file to a specified location in Databricks DBFS.</p> <p>Parameters:</p> Name Type Description Default <code>workspace</code> <code>WorkspaceClient</code> <p>The Databricks workspace client for interacting with DBFS.</p> required <code>path_dbfs</code> <code>str</code> <p>The destination path in Databricks DBFS.</p> required <code>data</code> <code>list[dict]</code> <p>The data to be saved as a JSON file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If there is an issue with the workspace client or file saving.</p> Note <p>This function saves the provided data as a JSON file in Databricks DBFS.</p> Source code in <code>quotes_dab/request_quote.py</code> <pre><code>def save_to_storage(\n    workspace: WorkspaceClient, path_dbfs: str, data: list[dict]\n) -&gt; None:\n    \"\"\"\n    Save data as a JSON file to a specified location in Databricks DBFS.\n\n    Args:\n        workspace (WorkspaceClient): The Databricks workspace client for interacting with DBFS.\n        path_dbfs (str): The destination path in Databricks DBFS.\n        data (list[dict]): The data to be saved as a JSON file.\n\n    Returns:\n        None\n\n    Raises:\n        AttributeError: If there is an issue with the workspace client or file saving.\n\n    Note:\n        This function saves the provided data as a JSON file in Databricks DBFS.\n    \"\"\"\n    if data is not None:\n        json_formatted = json.dumps(data)\n        json_datetime = f\"{path_dbfs}/data_json_{datetime.now().timestamp()}\"\n        try:\n            workspace.dbutils.fs.put(json_datetime, json_formatted)\n            logger.info(\"Saved to %s\", path_dbfs)\n        except AttributeError as e:\n            logger.error(e)\n    else:\n        logger.info(\"Data returned None\")\n</code></pre>"}]}